{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uBg6rF_E2UZ",
        "outputId": "7f9d19f0-a123-4655-9d34-ab6488b69594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlxtend"
      ],
      "metadata": {
        "id": "fiYUgpd3FImQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PAdpfTm104f",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from itertools import combinations\n",
        "from sklearn.cluster import *\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('dataset_clean.csv')\n",
        "# df['Order Date'] = pd.to_datetime(df['Order Date'])\n",
        "# df['year'] = df['Order Date'].dt.year\n",
        "# df.drop('Order Date', axis=1, inplace=True)  # Drop the original datetime column after extraction\n",
        "# df"
      ],
      "metadata": {
        "id": "euayPGZf7Qmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_cols_before = df.shape[1]\n",
        "df = df.drop(['Order Zipcode','Shipping Date','Customer Zipcode'], axis=1)\n",
        "num_cols_after = df.shape[1]\n",
        "num_cols_dropped = num_cols_before - num_cols_after\n",
        "print(\"Number of columns dropped:\", num_cols_dropped)"
      ],
      "metadata": {
        "id": "8-KA9RVDYAQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = df.columns\n",
        "print(column_names)"
      ],
      "metadata": {
        "id": "t-OyUzsYYiMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.api.types import is_numeric_dtype\n",
        "\n",
        "numeric_columns = [col for col in df.select_dtypes(include=np.number) if col not in df.filter(like='non_numeric')]\n",
        "numeric_columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwx4Eoyo74zC",
        "outputId": "01b42d7f-3071-4c7d-bcf2-e5f589f9a8d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Days for shipping (real)',\n",
              " 'Days for shipment (scheduled)',\n",
              " 'Sales per customer',\n",
              " 'Late_delivery_risk',\n",
              " 'Category Id',\n",
              " 'Customer Id',\n",
              " 'Customer Zipcode',\n",
              " 'Department Id',\n",
              " 'Latitude',\n",
              " 'Longitude',\n",
              " 'Order Customer Id',\n",
              " 'Order Id',\n",
              " 'Order Item Cardprod Id',\n",
              " 'Order Item Discount',\n",
              " 'Order Item Discount Rate',\n",
              " 'Order Item Id',\n",
              " 'Order Item Profit Ratio',\n",
              " 'Order Item Quantity',\n",
              " 'Sales',\n",
              " 'Order Profit Per Order',\n",
              " 'Product Card Id',\n",
              " 'Product Category Id',\n",
              " 'Product Price',\n",
              " 'Product Status']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_column_dtypes(df):\n",
        "  dtype_dict = {}\n",
        "  for col in df.columns:\n",
        "    # Get the data type\n",
        "    dtype = df[col].dtype\n",
        "    # Check if the data type is numeric\n",
        "    if not is_numeric_dtype(dtype):\n",
        "      dtype_dict[col] = 'category'\n",
        "    else:\n",
        "      # Remove 'dtype(' and ')' from the string representation\n",
        "      dtype = str(dtype).strip(\"dtype(')\").strip(\")\")\n",
        "      dtype_dict[col] = dtype\n",
        "  return dtype_dict\n",
        "\n",
        "\n",
        "data_types = get_column_dtypes(df)\n",
        "print(data_types)"
      ],
      "metadata": {
        "id": "wVMSKbnU8-D4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lf5m6lR-2myG",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('dataset_clean.csv', delimiter=',', dtype=data_types)\n",
        "\n",
        "numerical_features = numeric_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2tWdWek7f0w",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_urgency(shipping_mode):\n",
        "    if shipping_mode in ['Same Day']:\n",
        "        return 'High'\n",
        "    elif shipping_mode in ['First Class', 'Second Class']:\n",
        "        return 'Medium'\n",
        "    else:\n",
        "        return 'Low'\n",
        "\n",
        "data['URGENCY'] = data['Shipping Mode'].apply(assign_urgency)"
      ],
      "metadata": {
        "id": "KBg2YX2s_9JM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows with missing values\n",
        "num_cols_before = data.shape[1]\n",
        "data = data.dropna()\n",
        "num_cols_after = data.shape[1]\n",
        "num_cols_dropped = num_cols_before - num_cols_after\n",
        "print(\"Number of columns dropped:\", num_cols_dropped)"
      ],
      "metadata": {
        "id": "VMxc8V2-Ng5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "uYsNVtGq4o7F"
      },
      "source": [
        "## Generate combinations of numerical features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "sK1xLl2R4o7I"
      },
      "outputs": [],
      "source": [
        "feature_comb = list(combinations(numerical_features, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "f-2FIoCN4o7I"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def make_cluster(algorithm, data, features, figsize, **kwargs):\n",
        "    fig, (ax1, ax2) = plt.subplots(2, figsize=figsize)\n",
        "    fig.suptitle('Supply Chain Clustering')\n",
        "    sns.scatterplot(ax=ax1, data=data, x=features[0], y=features[1], hue='Category Id', palette='viridis')\n",
        "    ax1.set(title='Ground Truth', xlabel=features[0], ylabel=features[1])\n",
        "    algorithm_instance = algorithm(**kwargs)\n",
        "    algorithm_name = type(algorithm_instance).__name__\n",
        "    data['Cluster'] = algorithm_instance.fit_predict(data[features])\n",
        "    sns.scatterplot(ax=ax2, data=data, x=features[0], y=features[1], hue='Cluster', palette='viridis')\n",
        "    ax2.set(title=f\"Clustering by {algorithm_name}\", xlabel=features[0], ylabel=features[1])\n",
        "    filename = f\"{algorithm_name}_{'_'.join(features)}.png\"\n",
        "    # Save the figure with the generated filename\n",
        "    # Create the 'figures' directory if it doesn't exist\n",
        "    if not os.path.exists(\"./figures\"):\n",
        "      os.makedirs(\"./figures\")\n",
        "    plt.savefig(\"./figures/\" + filename)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def make_cluster(algorithm, data, features, figsize, **kwargs):\n",
        "    fig, ax2 = plt.subplots(figsize=figsize)\n",
        "\n",
        "    fig.suptitle('Supply Chain Clustering')\n",
        "\n",
        "    algorithm_instance = algorithm(**kwargs)\n",
        "    algorithm_name = type(algorithm_instance).__name__\n",
        "    data['Cluster'] = algorithm_instance.fit_predict(data[features])\n",
        "\n",
        "    sns.scatterplot(ax=ax2, data=data, x=features[0], y=features[1], hue='Cluster', palette='viridis')\n",
        "    ax2.set(title=f\"Clustering by {algorithm_name}\", xlabel=features[0], ylabel=features[1])\n",
        "\n",
        "    filename = f\"{algorithm_name}_{'_'.join(features)}.png\"\n",
        "\n",
        "    # Save the figure with the generated filename\n",
        "    # Create the 'figures' directory if it doesn't exist\n",
        "    if not os.path.exists(\"./figures\"):\n",
        "        os.makedirs(\"./figures\")\n",
        "    plt.savefig(\"./figures/\" + filename)\n",
        "\n",
        "    # Clear the plot to avoid memory issues if plotting multiple clusters\n",
        "    plt.clf()  # Clear the figure to prevent memory buildup\n",
        "\n"
      ],
      "metadata": {
        "id": "QQKJ9XQQ-Cfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dksZ_JMMCv6A",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## DBSCAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyD-nWW8En6-",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "for fc in feature_comb:\n",
        "    features = [fc[0], fc[1]]\n",
        "    make_cluster(DBSCAN, data, features, (12, 10), eps=3, min_samples=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "pJ3A11hu4o7P"
      },
      "source": [
        "## Kmeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eON3Labu9LLI",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "for fc in feature_comb:\n",
        "    features = [fc[0], fc[1]]\n",
        "    make_cluster(KMeans, data, features, (12, 10), n_clusters=3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "K1S9rYTD4o7R"
      },
      "source": [
        "## Agglomerative Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "p8qNht3k4o7T"
      },
      "outputs": [],
      "source": [
        "for fc in feature_comb:\n",
        "    features = ['Category Id', fc[0]]\n",
        "    scaler = StandardScaler()\n",
        "    data_scaled = scaler.fit_transform(data[features])\n",
        "    make_cluster(AgglomerativeClustering, data, features, (12, 10), n_clusters=3, metric='euclidean', linkage='ward')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "4iHX0hm74o7T"
      },
      "source": [
        "## Apriori"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "delivery = []\n",
        "category_name = []\n",
        "order_region = []\n",
        "order_status = []\n",
        "for suffix in df['Delivery Status'].unique():\n",
        "  delivery.append(f\"Delivery Status_{suffix}\")\n",
        "\n",
        "for suffix in df['Category Name'].unique():\n",
        "  category_name.append(f\"Category Name_{suffix}\")\n",
        "\n",
        "for suffix in df['Order Region'].unique():\n",
        "  order_region.append(f\"Order Region_{suffix}\")\n",
        "\n",
        "\n",
        "for suffix in df['Order Status'].unique():\n",
        "  order_status.append(f\"Order Status_{suffix}\")\n",
        "print(len(delivery))\n",
        "print(len(category_name))\n",
        "print(len(order_region))\n",
        "print(len(order_status))"
      ],
      "metadata": {
        "id": "slopH-A9fvTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "combinations = product(delivery,order_region,order_status)\n",
        "list_of_tuples = []\n",
        "for combination in combinations:\n",
        "  new_list = list(combination)\n",
        "  list_of_tuples.append(new_list)\n",
        "  print('\\n')\n",
        "\n",
        "final_combination = []\n",
        "for new_list in list_of_tuples:\n",
        "  for category in category_name:\n",
        "    final_combination.append(new_list + [category])\n",
        "\n",
        "for combination in final_combination:\n",
        "  print(list(combination))\n",
        "  print('\\n')"
      ],
      "metadata": {
        "id": "m_gX6eY4hWCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "wNPKdf6G4o7U"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "df.drop(df.columns.difference(['Category Name','Delivery Status','Order Region','Order Status']), axis=1, inplace=True)\n",
        "\n",
        "df_apriori = pd.get_dummies(df)\n",
        "\n",
        "df_apriori.head()\n",
        "results = []\n",
        "for combination in final_combination:\n",
        "  columns=combination\n",
        "  # Apply Apriori algorithm\n",
        "  frequent_itemsets = apriori(df_apriori[columns], min_support=0.05, use_colnames=True)\n",
        "  if len(frequent_itemsets) < 1:\n",
        "    print(f\"No rules found for {columns}\")\n",
        "    continue\n",
        "  # Generate association rules\n",
        "  association_rules_df = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
        "\n",
        "  # Sort rules by lift\n",
        "  association_rules_df = association_rules_df.sort_values(by='lift', ascending=False)\n",
        "\n",
        "  # Print the top 10 interesting patterns\n",
        "  top_patterns = association_rules_df.head(10)\n",
        "  results.append(top_patterns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, df in enumerate(results):\n",
        "  filename = f\"/content/apriori/top_patterns_{i}.csv\"\n",
        "  df.to_csv(filename, index=False)"
      ],
      "metadata": {
        "id": "zYqJ9odFzKO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "1qOExgFl3ks3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}